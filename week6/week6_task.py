# -*- coding: utf-8 -*-
"""Week6_task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VKDK1RZwM2XL6qUvo2-58q_13RDVDwep
"""

!pip install kaggle
from google.colab import files
files.upload()

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/root/.kaggle"
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d rzykov/uk-corporate-data-company-house-2023

!ls

!unzip uk-corporate-data-company-house-2023.zip

!pip install dask[complete] modin[all] ray

import pandas as pd
import dask.dataframe as dd
import modin.pandas as mpd

import time

# Pandas timing
start = time.time()
df_pandas = pd.read_csv('corporate_uk/companies.csv',delimiter=';')
end = time.time()
print(f"Pandas time: {end - start} seconds")

# Dask timing
start = time.time()
df_dask = dd.read_csv('corporate_uk/companies.csv',delimiter=';')
end = time.time()
print(f"Dask time: {end - start} seconds")

# Modin timing
start = time.time()
df_modin = mpd.read_csv('corporate_uk/companies.csv',delimiter=';')
end = time.time()
print(f"Modin time: {end - start} seconds")

import ray

ray.init(ignore_reinit_error=True)

# Using Ray directly
import ray.data
# Ray timing
start = time.time()
df_ray = ray.data.read_csv('corporate_uk/companies.csv',delimiter=';')
end = time.time()
print(f"Ray time: {end - start} seconds")

"""## Clean column names"""

df_pandas.columns = df_pandas.columns.str.replace('[^A-Za-z0-9_]', '', regex=True).str.strip()

# For Dask, Modin, and Ray (this step should work similarly with Dask/Modin/Ray DataFrames)
df_dask.columns = df_dask.columns.str.replace('[^A-Za-z0-9_]', '', regex=True).str.strip()
df_modin.columns = df_modin.columns.str.replace('[^A-Za-z0-9_]', '', regex=True).str.strip()

"""## Create utility file"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile utility.py
# import logging
# import os
# import subprocess
# import yaml
# import pandas as pd
# import datetime
# import gc
# import re
# 
# ##############
# # Utilities  #
# ##############
# 
# def read_config_file(filepath):
#   # Reads a YAML configuration file and returns the parsed content.
#     if not os.path.exists(filepath):
#         logging.error(f"Config file not found: {filepath}")
#         raise FileNotFoundError(f"Config file not found: {filepath}")
#     with open(filepath, 'r') as stream:
#         try:
#             return yaml.safe_load(stream)
#         except yaml.YAMLError as exc:
#             logging.error(f"Error reading YAML file: {exc}")
#             raise
# 
# def replacer(string, char):
#   # Replaces consecutive occurrences of a character in a string with a single instance.
#     pattern = char + '{2,}'
#     string = re.sub(pattern, char, string)
#     return string
# 
# ##################
# # Data Validation #
# ##################
# 
# def col_header_val(df, table_config):
#     logging.info("Starting column header validation.")
# 
#     df.columns = df.columns.str.lower()
#     df.columns = df.columns.str.replace(r'[^\w]', '_', regex=True)
#     df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))
#     df.columns = list(map(lambda x: replacer(x, '_'), list(df.columns)))
# 
#     expected_col = list(map(lambda x: x.lower(), table_config['columns']))
#     expected_col.sort()
# 
#     df.columns = list(map(lambda x: x.lower(), list(df.columns)))
#     df = df.reindex(sorted(df.columns), axis=1)
# 
#     if len(df.columns) == len(expected_col) and list(expected_col) == list(df.columns):
#         logging.info("Column name and length validation passed.")
#         print("Column name and column length validation passed.")
#         return 1
#     else:
#         logging.warning("Column name and column length validation failed.")
#         print("Column name and column length validation failed.")
# 
#         mismatched_columns_file = list(set(df.columns).difference(expected_col))
#         missing_YAML_file = list(set(expected_col).difference(df.columns))
# 
#         print("Following File columns are not in the YAML file:", mismatched_columns_file)
#         print("Following YAML columns are not in the uploaded file:", missing_YAML_file)
# 
#         logging.error(f"File columns not in YAML: {mismatched_columns_file}")
#         logging.error(f"YAML columns missing in file: {missing_YAML_file}")
# 
#         return 0
# 
# ####################
# # File Processing  #
# ####################
# 
# def read_large_file(filepath, sep=',', chunksize=10**6):
#     logging.info(f"Reading file: {filepath}")
#     try:
#         chunk_list = []
#         for chunk in pd.read_csv(filepath, sep=sep, chunksize=chunksize):
#             chunk_list.append(chunk)
#         logging.info(f"File read successfully: {filepath}")
#         return pd.concat(chunk_list, axis=0)
#     except Exception as e:
#         logging.error(f"Error reading file {filepath}: {e}")
#         raise
# 
# def write_file(df, filepath, sep='|', compression='gzip'):
#     logging.info(f"Writing file to {filepath}")
#     try:
#         df.to_csv(filepath, sep=sep, index=False, compression=compression)
#         logging.info(f"File written successfully: {filepath}")
#     except Exception as e:
#         logging.error(f"Error writing file {filepath}: {e}")
#         raise
# 
# ####################
# # Summary Function #
# ####################
# 
# def generate_file_summary(df, filepath):
#     total_rows, total_columns = df.shape
#     file_size = os.path.getsize(filepath)
# 
#     summary = {
#         "Total Rows": total_rows,
#         "Total Columns": total_columns,
#         "File Size (MB)": file_size / (1024 ** 2)
#     }
# 
#     print(f"Summary: {summary}")
#     logging.info(f"File Summary: {summary}")
#     return summary
#

"""## Write yaml file"""

df_pandas.columns

# Commented out IPython magic to ensure Python compatibility.
# %%writefile file.yaml
# file_type: csv
# dataset_name: uk-corporate-data-company-house-2023
# file_name: companies
# table_name: companies_tab
# inbound_delimiter: ";"
# outbound_delimiter: "|"
# skip_leading_rows: 1
# columns:
#     - company_number
#     - company_type
#     - office_address
#     - incorporation_date
#     - jurisdiction
#     - company_status
#     - account_type
#     - company_name
#     - sic_codes
#     - date_of_cessation
#     - next_accounts_overdue
#     - confirmation_statement_overdue
#     - owners
#     - places
#     - average_number_employees_during_period
#     - current_assets
#     - last_accounts_period_end
#     - city_name
#

!ls

!pip install pyyaml

!pip install python-utils

import utility as util
config_data = util.read_config_file("file.yaml")
config_data

config_data['inbound_delimiter']

#read file using config file
file_type = config_data['file_type']
source_file = "corporate_uk/" + config_data['file_name'] + f'.{file_type}'
df = pd.read_csv(source_file, delimiter=config_data['inbound_delimiter'])
df.head()

"""## validate"""

util.col_header_val(df,config_data)

print("columns of files are:" ,df.columns)
print("columns of YAML are:" ,config_data['columns'])

if util.col_header_val(df,config_data)==0:
    print("validation failed")
    # write code to reject the file
else:
    print("col validation passed")

import csv
import gzip

from dask import dataframe as dd
df = dd.read_csv('corporate_uk/companies.csv',delimiter='\t')

# Write csv in gz format in pipe separated text file (|)
df.to_csv('companies.csv.gz',
          sep='|',
          header=True,
          index=False,
          quoting=csv.QUOTE_ALL,
          compression='gzip',
          quotechar='"',
          doublequote=True)

# Get file summary
file_size = os.path.getsize('companies.csv.gz')
num_rows = len(df)
num_cols = len(df.columns)

# Print file summary
print("File summary:")
print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_cols}")
print(f"File size: {file_size} bytes")

!ls

